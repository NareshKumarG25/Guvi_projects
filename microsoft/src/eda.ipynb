{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1= r'D:\\Naresh\\GUVI\\Projects\\Microsoft\\raw_data\\GUIDE_Train.csv'\n",
    "file2 = r'D:\\Naresh\\GUVI\\Projects\\Microsoft\\raw_data\\GUIDE_Test.csv'\n",
    "file3=r'D:\\Naresh\\GUVI\\Projects\\Microsoft\\raw_data\\new_train_sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# train = pd.read_csv(file1)\n",
    "train_2=pd.read_csv(file3)\n",
    "test = pd.read_csv(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of train 2 : 4758418\n"
     ]
    }
   ],
   "source": [
    "# print(f'Lenght of train : {len(train)}')\n",
    "print(f'Lenght of train 2 : {len(train_2)}')\n",
    "print(f'Lenght of test : {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.DataFrame\n",
    "train_2_df =pd.DataFrame\n",
    "test_df=pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.drop(columns=['Timestamp'], inplace=True)\n",
    "train_2.drop(columns=['Timestamp'], inplace=True)\n",
    "test.drop(columns=['Timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tra = train_2\n",
    "nan_count = tra.isna().sum()\n",
    "nan_percentage = (nan_count / len(tra)) * 100\n",
    "duplicates_in_columns = tra.apply(lambda col: col.duplicated(keep=False).any())\n",
    "# variance_data = tra.var()\n",
    "summary = pd.DataFrame({\n",
    "    'NaN Count': nan_count,\n",
    "    'NaN Percentage': nan_percentage,\n",
    "    'Duplicates' : duplicates_in_columns,\n",
    "    # 'variance_data' :variance_data\n",
    "})\n",
    "\n",
    "train_2_df = summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing data in excel...\n"
     ]
    }
   ],
   "source": [
    "output_path=\"D:/Naresh/GUVI/Projects/Microsoft/data/processed_data/\"\n",
    "\n",
    "with pd.ExcelWriter(output_path+'analysis_data.xlsx') as writer:\n",
    "        print(\"writing data in excel...\")\n",
    "        # train_df.to_excel(writer, sheet_name='Train 1',index=True)\n",
    "        train_2_df.to_excel(writer, sheet_name='Train 2',index=True)\n",
    "        # test_df.to_excel(writer, sheet_name='Test',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = nan_percentage[nan_percentage < 50].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered=train_2[columns_to_keep]\n",
    "df_filtered = df_filtered[df_filtered['IncidentGrade'].notna()]\n",
    "\n",
    "df_test = test[columns_to_keep]\n",
    "df_test=df_test[df_test['IncidentGrade'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features=[]\n",
    "numerical_features =[]\n",
    "for i in df_filtered.columns:\n",
    "\n",
    "    if type(df_filtered[i][0])== str:\n",
    "        categorical_features.append(i)\n",
    "    else:\n",
    "        numerical_features.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:573: ConstantInputWarning: Each of the input arrays is constant; the F statistic is not defined or infinite\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA Results for Numerical Features:\n",
      "Unnamed: 0: F-statistic = 0.22925829642951948, p-value = 0.7951231382876303\n",
      "Id: F-statistic = 1439.484198513051, p-value = 0.0\n",
      "OrgId: F-statistic = 58997.76021886329, p-value = 0.0\n",
      "IncidentId: F-statistic = 76452.55955725766, p-value = 0.0\n",
      "AlertId: F-statistic = 47081.29268933218, p-value = 0.0\n",
      "DetectorId: F-statistic = 32908.98954426741, p-value = 0.0\n",
      "AlertTitle: F-statistic = 48290.78906666099, p-value = 0.0\n",
      "DeviceId: F-statistic = 15635.311122018205, p-value = 0.0\n",
      "Sha256: F-statistic = 47594.834053318205, p-value = 0.0\n",
      "IpAddress: F-statistic = 58524.64720546779, p-value = 0.0\n",
      "Url: F-statistic = 21112.06672260003, p-value = 0.0\n",
      "AccountSid: F-statistic = 24636.7169202503, p-value = 0.0\n",
      "AccountUpn: F-statistic = 4759.04937297763, p-value = 0.0\n",
      "AccountObjectId: F-statistic = 26743.9579418214, p-value = 0.0\n",
      "AccountName: F-statistic = 17730.550232143647, p-value = 0.0\n",
      "DeviceName: F-statistic = 50484.52016148558, p-value = 0.0\n",
      "NetworkMessageId: F-statistic = 63803.88275238493, p-value = 0.0\n",
      "RegistryKey: F-statistic = 8767.964467241332, p-value = 0.0\n",
      "RegistryValueName: F-statistic = 217.63178162648973, p-value = 3.076551888558765e-95\n",
      "RegistryValueData: F-statistic = 404.24902410506155, p-value = 2.8305701665374938e-176\n",
      "ApplicationId: F-statistic = 1467.8178662128046, p-value = 0.0\n",
      "ApplicationName: F-statistic = 1440.178631903879, p-value = 0.0\n",
      "OAuthApplicationId: F-statistic = 105.93472762386102, p-value = 9.866477532828021e-47\n",
      "FileName: F-statistic = 62598.092000631914, p-value = 0.0\n",
      "FolderPath: F-statistic = 63173.555928302405, p-value = 0.0\n",
      "ResourceIdName: F-statistic = 863.3021208386185, p-value = 0.0\n",
      "OSFamily: F-statistic = 10039.212454464861, p-value = 0.0\n",
      "OSVersion: F-statistic = 10061.126800280788, p-value = 0.0\n",
      "CountryCode: F-statistic = 228965.7741272291, p-value = 0.0\n",
      "State: F-statistic = 188291.76701489132, p-value = 0.0\n",
      "City: F-statistic = 190443.64795173952, p-value = 0.0\n",
      "target_encoded: F-statistic = inf, p-value = 0.0\n",
      "\n",
      "Chi-Square Results for Categorical Features:\n",
      "Category: Chi2-statistic = 891259.7707364529, p-value = 0.0\n",
      "IncidentGrade: Chi2-statistic = 9465285.999999998, p-value = 0.0\n",
      "EntityType: Chi2-statistic = 895389.8854232655, p-value = 0.0\n",
      "EvidenceRole: Chi2-statistic = 121913.79393155336, p-value = 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#Label encode the IncidentGrade (if necessary) since it's categorical\n",
    "target_encoder = LabelEncoder()\n",
    "# df_filtered['target_encoded'] = target_encoder.fit_transform(df_filtered['IncidentGrade'])\n",
    "\n",
    "#  ANOVA for numerical features with categorical IncidentGrade\n",
    "anova_results = {}\n",
    "\n",
    "for feature in numerical_features:\n",
    "    # Perform ANOVA for each numerical feature against the IncidentGrade\n",
    "    group_data = [df_filtered[feature][df_filtered['IncidentGrade'] == category] for category in df_filtered['IncidentGrade'].unique()]\n",
    "    f_stat, p_value = stats.f_oneway(*group_data)\n",
    "    anova_results[feature] = (f_stat, p_value)\n",
    "\n",
    "print(\"ANOVA Results for Numerical Features:\")\n",
    "for feature, (f_stat, p_value) in anova_results.items():\n",
    "    print(f\"{feature}: F-statistic = {f_stat}, p-value = {p_value}\")\n",
    "\n",
    "# Chi-Square for categorical features with categorical IncidentGrade\n",
    "\n",
    "chi2_results = {}\n",
    "\n",
    "for feature in categorical_features:\n",
    "    # Create contingency table for each categorical feature\n",
    "    contingency_table = pd.crosstab(df_filtered[feature], df_filtered['IncidentGrade'])\n",
    "    \n",
    "    # Perform Chi-Square test\n",
    "    chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "    chi2_results[feature] = (chi2_stat, p_value)\n",
    "\n",
    "print(\"\\nChi-Square Results for Categorical Features:\")\n",
    "for feature, (chi2_stat, p_value) in chi2_results.items():\n",
    "    print(f\"{feature}: Chi2-statistic = {chi2_stat}, p-value = {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove=[]\n",
    "for values in anova_results:\n",
    "    if anova_results[values][1] != 0.0:\n",
    "        columns_to_remove.append(values)\n",
    "for values in chi2_results:\n",
    "    if chi2_results[values][1] != 0.0:\n",
    "        columns_to_remove.append(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Category', 'IncidentGrade', 'EntityType', 'EvidenceRole']\n"
     ]
    }
   ],
   "source": [
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.drop(columns=columns_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "for i in categorical_features:\n",
    "    df_filtered[i]=le.fit_transform(df_filtered[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example data: X (features) and y (target)\n",
    "y = df_filtered['IncidentGrade']\n",
    "X = df_filtered.drop('IncidentGrade', axis=1)  # All features\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a Random Forest Classifier to calculate feature importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame to view the importance of each feature\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = rf.score(X_test, y_test)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "selected_features = ['OrgId', 'DetectorId','AlertTitle','AlertId','Category','CountryCode','EntityType']  # Example\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "# Fit a Random Forest Classifier and evaluate using cross-validation\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "scores = cross_val_score(rf, X_selected, y, cv=5)\n",
    "\n",
    "print(f\"Cross-validated scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
